# -*- coding: utf-8 -*-
"""
å•æ–‡ä»¶é¢„æµ‹è„šæœ¬ï¼š
1. åŠ è½½å·²è®­ç»ƒå¥½çš„ encoder å’Œ LightGBM
2. è¯»å–å•ä¸ª CSV æ–‡ä»¶
3. æŠ½å–â€œæ ‡é¢˜â€ â†’ embedding â†’ LightGBM é¢„æµ‹
4. åœ¨åŸæ–‡ä»¶ä¸­æ–°å¢â€œåŸºç¡€åˆ†æ•°â€åˆ—å¹¶ä¿å­˜
"""

import os
import numpy as np
import pandas as pd
import torch
from torch.utils.data import Dataset, DataLoader
from transformers import AutoTokenizer
from lightgbm import Booster
from tqdm import tqdm

# =============== é…ç½® ===============
MODEL_NAME = "shibing624/text2vec-base-chinese-sentence"
ENCODER_PATH = "../../../res/training_model/multitask_encoder.pt"
LGBM_PATH = "../../../res/training_model/multitask_lightgbm.txt"

INPUT_FILE = "../../../mid_result/training_data/merged_2w_scored_titles_LLM.csv"  # ğŸ”¹ æŒ‡å®šè¦é¢„æµ‹çš„æ–‡ä»¶
OUTPUT_FILE = "../../../mid_result/model_compare/multitask_sentiment_title_predictions.csv"  # ğŸ”¹ è¾“å‡ºè·¯å¾„

BATCH_SIZE = 64
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# =============== æ•°æ®é›† ===============
class TitleDataset(Dataset):
    def __init__(self, texts, tokenizer, max_len=32):
        self.texts = texts
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        enc = self.tokenizer(
            self.texts[idx],
            max_length=self.max_len,
            truncation=True,
            padding="max_length",
            return_tensors="pt"
        )
        item = {k: v.squeeze(0) for k, v in enc.items()}
        item["text"] = self.texts[idx]
        return item

# =============== æ¨¡å‹ç»“æ„ ===============
import torch.nn as nn
from transformers import AutoModel

class MultiTaskModel(nn.Module):
    def __init__(self, model_name, hidden_size=768):
        super().__init__()
        self.encoder = AutoModel.from_pretrained(model_name)
        self.dropout = nn.Dropout(0.1)
        self.head_word = nn.Linear(hidden_size, 1)
        self.head_title = nn.Linear(hidden_size, 1)

    def forward(self, input_ids, attention_mask, task_type="title"):
        out = self.encoder(input_ids=input_ids, attention_mask=attention_mask)
        pooled = out.pooler_output if out.pooler_output is not None else out.last_hidden_state[:, 0, :]
        pooled = self.dropout(pooled)
        return self.head_title(pooled)

    def get_embedding(self, input_ids, attention_mask):
        out = self.encoder(input_ids=input_ids, attention_mask=attention_mask)
        pooled = out.pooler_output if out.pooler_output is not None else out.last_hidden_state[:, 0, :]
        return pooled.detach()

# =============== åŠ è½½æ¨¡å‹ ===============
print(f"ğŸ”¹ å½“å‰é¢„æµ‹è®¾å¤‡: {device}")
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

encoder = MultiTaskModel(MODEL_NAME).to(device)
encoder.load_state_dict(torch.load(ENCODER_PATH, map_location=device))
encoder.eval()

lgbm_model = Booster(model_file=LGBM_PATH)

# =============== å·¥å…·å‡½æ•° ===============
def get_embeddings(model, dataset, batch_size=64):
    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)
    all_vecs = []
    with torch.no_grad():
        for batch in tqdm(loader, desc="Encoding embeddings"):
            input_ids = batch["input_ids"].to(device)
            mask = batch["attention_mask"].to(device)
            vecs = model.get_embedding(input_ids, mask)
            all_vecs.append(vecs.cpu().numpy())
    return np.vstack(all_vecs)

def predict_file(file_path, save_path):
    print(f"ğŸ“‚ æ­£åœ¨å¤„ç†æ–‡ä»¶: {file_path}")
    df = pd.read_csv(file_path)

    if "æ ‡é¢˜" not in df.columns:
        raise ValueError(f"âŒ æœªæ‰¾åˆ°åˆ— 'æ ‡é¢˜'ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶åˆ—åã€‚å½“å‰åˆ—ä¸º: {df.columns.tolist()}")

    texts = df["æ ‡é¢˜"].astype(str).tolist()
    dataset = TitleDataset(texts, tokenizer, max_len=32)
    X = get_embeddings(encoder, dataset, batch_size=BATCH_SIZE)
    y_pred = lgbm_model.predict(X)

    df["åŸºç¡€åˆ†æ•°"] = y_pred
    os.makedirs(os.path.dirname(save_path), exist_ok=True)
    df.to_csv(save_path, index=False, encoding="utf-8-sig")
    print(f"âœ… é¢„æµ‹å®Œæˆï¼Œç»“æœå·²ä¿å­˜è‡³: {save_path}")

# =============== ä¸»ç¨‹åº ===============
if __name__ == "__main__":
    predict_file(INPUT_FILE, OUTPUT_FILE)
