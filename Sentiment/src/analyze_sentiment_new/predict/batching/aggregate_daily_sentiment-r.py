# -*- coding: utf-8 -*-
"""
æ‰¹é‡è®¡ç®—æ—¥åº¦æƒ…ç»ªæŒ‡æ•°
ç›®å½•ç»“æ„:
    ä¸»ç›®å½•/
        è‚¡ç¥¨ä»£ç 1/
            titles_with_weighted_score.csv
        è‚¡ç¥¨ä»£ç 2/
            titles_with_weighted_score.csv
è¾“å‡º: æ¯ä¸ªå­ç›®å½•ä¸‹æ–°å¢ daily_sentiment_index.csv
"""

import pandas as pd
import numpy as np
import os

# ========== é…ç½® ==========
INPUT_ROOT = "../../../../mid_result/hs300_data/stocks_separate_data"
INPUT_FILENAME = "titles_with_weighted_score.csv"
OUTPUT_FILENAME = "daily_sentiment_index.csv"
NORMALIZE_METHOD = "tanh_zscore"  # å¯é€‰: "tanh_zscore", "robust", "minmax"

# ========== å½’ä¸€åŒ–å‡½æ•° ==========
def normalize_series(series, method=NORMALIZE_METHOD):
    s = series.copy().astype(float)

    if method == "tanh_zscore":
        mu, sigma = s.mean(), s.std(ddof=0)
        if sigma == 0:
            return pd.Series(0, index=s.index)
        z = (s - mu) / sigma
        return np.tanh(0.5 * z)

    elif method == "robust":
        clip_percentile = 0.05
        q_low, q_high = s.quantile(clip_percentile), s.quantile(1 - clip_percentile)
        s_clipped = np.clip(s, q_low, q_high)
        return 2 * (s_clipped - q_low) / (q_high - q_low) - 1

    elif method == "minmax":
        min_val, max_val = s.min(), s.max()
        if max_val == min_val:
            return pd.Series(0, index=s.index)
        return 2 * (s - min_val) / (max_val - min_val) - 1

    else:
        raise ValueError(f"æœªçŸ¥å½’ä¸€åŒ–æ–¹æ³•: {method}")

# ========== ä¸»ç¨‹åº ==========
start_date = pd.to_datetime("2024-01-01")
end_date = pd.to_datetime("2025-02-28")

for stock_dir in os.listdir(INPUT_ROOT):
    full_dir = os.path.join(INPUT_ROOT, stock_dir)
    if not os.path.isdir(full_dir):
        continue

    input_path = os.path.join(full_dir, INPUT_FILENAME)
    output_path = os.path.join(full_dir, OUTPUT_FILENAME)

    if not os.path.exists(input_path):
        print(f"âš ï¸ æ–‡ä»¶ä¸å­˜åœ¨: {input_path}")
        continue

    print(f"\nğŸ“˜ æ­£åœ¨å¤„ç†: {input_path}")
    df = pd.read_csv(input_path)
    df["çº æ­£åæ—¶é—´"] = pd.to_datetime(df["çº æ­£åæ—¶é—´"], errors="coerce")
    df = df.dropna(subset=["çº æ­£åæ—¶é—´"])

    # ===== æˆªæ–­æ—¶é—´ =====
    df = df[(df["çº æ­£åæ—¶é—´"] >= start_date) & (df["çº æ­£åæ—¶é—´"] <= end_date)].reset_index(drop=True)
    if df.empty:
        print(f"âš ï¸ {stock_dir} æ•°æ®åœ¨æŒ‡å®šæ—¥æœŸèŒƒå›´ä¸ºç©ºï¼Œè·³è¿‡")
        continue

    # æŒ‰æ—¥èšåˆ
    daily = (
        df.groupby(df["çº æ­£åæ—¶é—´"].dt.date)
        .agg(æ—¥åº¦åŠ æƒæƒ…ç»ªæ€»å’Œ=("åŠ æƒæƒ…ç»ªè´¡çŒ®", "sum"),
             å¸–å­æ•°=("æ ‡é¢˜", "count"))
        .reset_index()
        .rename(columns={"çº æ­£åæ—¶é—´": "æ—¥æœŸ"})
    )

    # è®¡ç®—æŒ‡æ•°1
    daily["æƒ…ç»ªå‡å€¼"] = daily["æ—¥åº¦åŠ æƒæƒ…ç»ªæ€»å’Œ"] / daily["å¸–å­æ•°"]
    daily["æŒ‡æ•°1_å¼ºåº¦*çƒ­åº¦"] = daily["æƒ…ç»ªå‡å€¼"] * np.log1p(daily["å¸–å­æ•°"])

    # å½’ä¸€åŒ–
    daily["å½’ä¸€åŒ–æƒ…ç»ªå› å­"] = normalize_series(daily["æŒ‡æ•°1_å¼ºåº¦*çƒ­åº¦"], method=NORMALIZE_METHOD)

    # ä¿å­˜ç»“æœ
    daily.to_csv(output_path, index=False, encoding="utf-8-sig")
    print(f"âœ… å·²ä¿å­˜: {output_path}")

print("\nğŸ¯ æ‰€æœ‰å­ç›®å½•å·²å¤„ç†å®Œæˆï¼")

