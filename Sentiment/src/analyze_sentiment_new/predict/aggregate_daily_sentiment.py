# -*- coding: utf-8 -*-
"""
åŠŸèƒ½ï¼š
1. è¯»å–å¸–å­çº§æ•°æ®ï¼›
2. æŒ‰æ—¥èšåˆè®¡ç®—â€œæŒ‡æ•°1_å¼ºåº¦*çƒ­åº¦â€ï¼›
3. å¯¹æŒ‡æ•°1è¿›è¡Œå½’ä¸€åŒ–ï¼ˆèŒƒå›´[-1,1]ï¼ŒæŠ—å¼‚å¸¸å€¼ï¼‰ï¼›
4. æ‰¹é‡è¾“å‡ºæ—¥åº¦æƒ…ç»ªæŒ‡æ ‡æ–‡ä»¶ã€‚
"""

import pandas as pd
import numpy as np

# ========== ğŸ“‚ è¾“å…¥è¾“å‡ºè·¯å¾„åˆ—è¡¨ ==========
input_paths = [
    "../../../mid_result/hs300_data/hs300_stock_forum/titles_with_weighted_score.csv",
    "../../../mid_result/hs300_data/single_stock/titles_with_weighted_score.csv"
]

output_paths = [
    "../../../mid_result/hs300_data/hs300_stock_forum/daily_sentiment_index.csv",
    "../../../mid_result/hs300_data/single_stock/daily_sentiment_index.csv"
]

# ========== ğŸ§  å°è£…å½’ä¸€åŒ–å‡½æ•° ==========
def normalize_series(series, method="tanh_zscore"):
    """
    å¯¹åºåˆ—è¿›è¡Œå½’ä¸€åŒ–ï¼ˆé»˜è®¤è¾“å‡ºèŒƒå›´çº¦ä¸º [-1, 1]ï¼‰ã€‚
    å‚æ•°:
        series: pd.Series
        method: strï¼Œå¯é€‰
            - "tanh_zscore": å…ˆzscoreæ ‡å‡†åŒ–ï¼Œå†tanhå‹ç¼©ï¼ŒæŠ—å¼‚å¸¸å€¼ï¼ˆæ¨èï¼‰
            - "robust": æŒ‰åˆ†ä½æ•°è£å‰ªåçº¿æ€§æ˜ å°„åˆ°[-1,1]
            - "minmax": ç®€å•min-maxæ˜ å°„åˆ°[-1,1]
        clip_percentile: robustæ¨¡å¼ä¸‹çš„åˆ†ä½æ•°è£å‰ªæ¯”ä¾‹
    """
    s = series.copy().astype(float)

    if method == "tanh_zscore":
        mu, sigma = s.mean(), s.std(ddof=0)
        if sigma == 0:
            return pd.Series(0, index=s.index)
        z = (s - mu) / sigma
        return np.tanh(0.5 * z)

    elif method == "robust":
        clip_percentile = 0.05
        q_low, q_high = s.quantile(clip_percentile), s.quantile(1 - clip_percentile)
        s_clipped = np.clip(s, q_low, q_high)
        return 2 * (s_clipped - q_low) / (q_high - q_low) - 1

    elif method == "minmax":
        min_val, max_val = s.min(), s.max()
        if max_val == min_val:
            return pd.Series(0, index=s.index)
        return 2 * (s - min_val) / (max_val - min_val) - 1

    else:
        raise ValueError(f"æœªçŸ¥å½’ä¸€åŒ–æ–¹æ³•: {method}")





# ========== ğŸš€ éå†å¤„ç† ==========
for file_path, output_path in zip(input_paths, output_paths):
    print(f"\nğŸ“˜ æ­£åœ¨å¤„ç†æ–‡ä»¶ï¼š{file_path}")

    # ---------- 1. è¯»å–æ•°æ® ----------
    df = pd.read_csv(file_path)
    df["æ—¶é—´"] = pd.to_datetime(df["æ—¶é—´"], errors="coerce")
    df = df.dropna(subset=["æ—¶é—´"])

    # ---------- 2. æŒ‰æ—¥èšåˆ ----------
    daily = (
        df.groupby(df["æ—¶é—´"].dt.date)
        .agg(æ—¥åº¦åŠ æƒæƒ…ç»ªæ€»å’Œ=("åŠ æƒæƒ…ç»ªè´¡çŒ®", "sum"),
             å¸–å­æ•°=("æ ‡é¢˜", "count"))
        .reset_index()
        .rename(columns={"æ—¶é—´": "æ—¥æœŸ"})
    )

    # ---------- 3. è®¡ç®—æƒ…ç»ªæŒ‡æ ‡ ----------
    daily["æƒ…ç»ªå‡å€¼"] = daily["æ—¥åº¦åŠ æƒæƒ…ç»ªæ€»å’Œ"] / daily["å¸–å­æ•°"]
    daily["æŒ‡æ•°1_å¼ºåº¦*çƒ­åº¦"] = daily["æƒ…ç»ªå‡å€¼"] * np.log1p(daily["å¸–å­æ•°"])

    # ---------- 4. è°ƒç”¨å½’ä¸€åŒ–å‡½æ•° ----------
    daily["å½’ä¸€åŒ–æƒ…ç»ªå› å­"] = normalize_series(daily["æŒ‡æ•°1_å¼ºåº¦*çƒ­åº¦"], method="tanh_zscore")

    # ---------- 5. ä¿å­˜ç»“æœ ----------
    daily.to_csv(output_path, index=False, encoding="utf-8-sig")
    print(f"âœ… å·²å®Œæˆï¼š{output_path}")

print("\nğŸ¯ æ‰€æœ‰æ–‡ä»¶å·²å¤„ç†å®Œæˆï¼")
