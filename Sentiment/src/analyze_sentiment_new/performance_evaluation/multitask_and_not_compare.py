# -*- coding: utf-8 -*-
"""
å•æ–‡ä»¶é¢„æµ‹è„šæœ¬ï¼š
1. åŠ è½½å·²è®­ç»ƒå¥½çš„ encoder å’Œ LightGBM
2. è¯»å–å•ä¸ª CSV æ–‡ä»¶
3. æŠ½å–â€œæ ‡é¢˜â€ â†’ embedding â†’ LightGBM é¢„æµ‹
4. åœ¨åŸæ–‡ä»¶ä¸­æ–°å¢â€œåŸºç¡€åˆ†æ•°â€åˆ—å¹¶ä¿å­˜
"""

import os
import numpy as np
import pandas as pd
import torch
from torch.utils.data import Dataset, DataLoader
from transformers import AutoTokenizer
from lightgbm import Booster
from tqdm import tqdm

# =============== é…ç½® ===============
MODEL_NAME = "shibing624/text2vec-base-chinese-sentence"
ENCODER_PATH = "../../../res/training_model/multitask_encoder.pt"
LGBM_PATH = "../../../res/training_model/multitask_lightgbm.txt"

SENTENCE_ONLY_MODEL_PATH = "shibing624/text2vec-base-chinese-sentence"  # ä»…å¥å­æ¨¡å‹è·¯å¾„
SENTENCE_ONLY_LGBM_PATH = "../../../res/training_model/sentence_only_lightgbm.txt"

INPUT_FILE = "../../../mid_result/training_data/merged_2w_scored_titles_LLM.csv"  # ğŸ”¹ æŒ‡å®šè¦é¢„æµ‹çš„æ–‡ä»¶
OUTPUT_FILE = "titles_with_score.csv"  # ğŸ”¹ è¾“å‡ºè·¯å¾„

BATCH_SIZE = 64
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# =============== æ•°æ®é›† ===============
class TitleDataset(Dataset):
    def __init__(self, texts, tokenizer, max_len=32):
        self.texts = texts
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        enc = self.tokenizer(
            self.texts[idx],
            max_length=self.max_len,
            truncation=True,
            padding="max_length",
            return_tensors="pt"
        )
        item = {k: v.squeeze(0) for k, v in enc.items()}
        item["text"] = self.texts[idx]
        return item

# =============== æ¨¡å‹ç»“æ„ ===============
import torch.nn as nn
from transformers import AutoModel

class MultiTaskModel(nn.Module):
    def __init__(self, model_name, hidden_size=768):
        super().__init__()
        self.encoder = AutoModel.from_pretrained(model_name)
        self.dropout = nn.Dropout(0.1)
        self.head_title = nn.Linear(hidden_size, 1)

    def forward(self, input_ids, attention_mask, task_type="title"):
        out = self.encoder(input_ids=input_ids, attention_mask=attention_mask)
        pooled = out.pooler_output if out.pooler_output is not None else out.last_hidden_state[:, 0, :]
        pooled = self.dropout(pooled)
        return self.head_title(pooled)

    def get_embedding(self, input_ids, attention_mask):
        out = self.encoder(input_ids=input_ids, attention_mask=attention_mask)
        pooled = out.pooler_output if out.pooler_output is not None else out.last_hidden_state[:, 0, :]
        return pooled.detach()

# =============== åŠ è½½æ¨¡å‹ ===============
print(f"ğŸ”¹ å½“å‰é¢„æµ‹è®¾å¤‡: {device}")
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

# åŠ è½½å¤šä»»åŠ¡æ¨¡å‹ï¼ˆMultiTaskModelï¼‰å’ŒLightGBMæ¨¡å‹
encoder = MultiTaskModel(MODEL_NAME).to(device)
encoder.load_state_dict(torch.load(ENCODER_PATH, map_location=device))
encoder.eval()
lgbm_model = Booster(model_file=LGBM_PATH)

# åŠ è½½ä»…å¥å­æ¨¡å‹å’Œå¯¹åº”çš„LightGBM
sentence_encoder = AutoModel.from_pretrained(SENTENCE_ONLY_MODEL_PATH).to(device)
sentence_encoder.eval()
sentence_lgbm_model = Booster(model_file=SENTENCE_ONLY_LGBM_PATH)

# =============== å·¥å…·å‡½æ•° ===============
def get_embeddings(model, dataset, batch_size=64):
    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)
    all_vecs = []
    with torch.no_grad():
        for batch in tqdm(loader, desc="Encoding embeddings"):
            input_ids = batch["input_ids"].to(device)
            mask = batch["attention_mask"].to(device)
            vecs = model.get_embedding(input_ids, mask)
            all_vecs.append(vecs.cpu().numpy())
    return np.vstack(all_vecs)

def predict_file(file_path, save_path):
    print(f"ğŸ“‚ æ­£åœ¨å¤„ç†æ–‡ä»¶: {file_path}")
    df = pd.read_csv(file_path)

    if "æ ‡é¢˜" not in df.columns:
        raise ValueError(f"âŒ æœªæ‰¾åˆ°åˆ— 'æ ‡é¢˜'ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶åˆ—åã€‚å½“å‰åˆ—ä¸º: {df.columns.tolist()}")

    texts = df["æ ‡é¢˜"].astype(str).tolist()
    dataset = TitleDataset(texts, tokenizer, max_len=32)

    # è·å–å¤šä»»åŠ¡æ¨¡å‹ï¼ˆMultiTaskModelï¼‰çš„åµŒå…¥å‘é‡å¹¶é¢„æµ‹
    X_multitask = get_embeddings(encoder, dataset, batch_size=BATCH_SIZE)
    y_pred_multitask = lgbm_model.predict(X_multitask)

    # è·å–ä»…å¥å­æ¨¡å‹çš„åµŒå…¥å‘é‡å¹¶é¢„æµ‹
    X_sentence_only = get_embeddings(sentence_encoder, dataset, batch_size=BATCH_SIZE)
    y_pred_sentence_only = sentence_lgbm_model.predict(X_sentence_only)

    # å°†ä¸¤ä¸ªæ¨¡å‹çš„é¢„æµ‹ç»“æœæ·»åŠ åˆ°åŸå§‹ DataFrame ä¸­
    df["MultiTask_åŸºç¡€åˆ†æ•°"] = y_pred_multitask
    df["SentenceOnly_åŸºç¡€åˆ†æ•°"] = y_pred_sentence_only

    # ä¿å­˜ç»“æœ
    os.makedirs(os.path.dirname(save_path), exist_ok=True)
    df.to_csv(save_path, index=False, encoding="utf-8-sig")
    print(f"âœ… é¢„æµ‹å®Œæˆï¼Œç»“æœå·²ä¿å­˜è‡³: {save_path}")

# =============== ä¸»ç¨‹åº ===============
if __name__ == "__main__":
    predict_file(INPUT_FILE, OUTPUT_FILE)
