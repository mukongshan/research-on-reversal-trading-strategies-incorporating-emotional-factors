# -*- coding: utf-8 -*-
"""
å¤šæ¨¡å‹æ¯”è¾ƒ + LightGBM å›å½’
é‡åˆ°æ— æ³•æ‹‰å–çš„æ¨¡å‹æ—¶è‡ªåŠ¨è·³è¿‡ï¼Œä¸ä¸­æ–­ç¨‹åº
"""

import os
import numpy as np
import pandas as pd
import torch
from transformers import AutoTokenizer, AutoModel
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from lightgbm import LGBMRegressor

# =============== å…¨å±€è®¾ç½® ===============
SEED = 42
np.random.seed(SEED)
torch.manual_seed(SEED)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# =============== è¾“å…¥ / è¾“å‡º / æ¨¡å‹é€‰æ‹© ===============
DATA_PATH = "../../../mid_result/training_data/merged_15K_scored_titles.csv"
SAVE_SUMMARY_PATH = "model_comparison_results.csv"

MODEL_LIST = [
    "shibing624/text2vec-base-chinese", # 0.4844
    "shibing624/text2vec-base-chinese-sentence", # 0.4933
    "shibing624/text2vec-base-chinese-paraphrase", # 0.4505
    "hfl/chinese-roberta-wwm-ext", # 0.4395
    "hfl/chinese-macbert-base", # 0.3679
    "nghuyong/ernie-3.0-base-zh", # 0.1793
    "bert-base-chinese" # 0.3639
]

# =============== æ•°æ®åŠ è½½ ===============
def load_data(data_path):
    if data_path.endswith(".csv"):
        df = pd.read_csv(data_path, encoding="utf-8-sig")
    else:
        df = pd.read_excel(data_path, engine="openpyxl")

    df.columns = df.columns.str.strip()
    df = df.dropna(subset=["æ ‡é¢˜", "åˆ†æ•°"]).copy()
    df["æ ‡é¢˜"] = df["æ ‡é¢˜"].astype(str)
    df["åˆ†æ•°"] = pd.to_numeric(df["åˆ†æ•°"], errors="coerce")
    df = df.dropna(subset=["åˆ†æ•°"])
    return df["æ ‡é¢˜"].tolist(), df["åˆ†æ•°"].to_numpy()

# =============== å‘é‡åŒ– ===============
def get_embeddings(texts, model_name, batch_size=64, max_len=32):
    try:
        print(f"\nâš™ï¸ æ­£åœ¨åŠ è½½æ¨¡å‹ {model_name} ...")
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = AutoModel.from_pretrained(model_name).to(device)
        model.eval()
    except Exception as e:
        print(f"âŒ æ¨¡å‹ {model_name} åŠ è½½å¤±è´¥: {e}")
        return None

    print(f"âœ… æ¨¡å‹ {model_name} åŠ è½½å®Œæˆï¼Œå¼€å§‹ç”Ÿæˆå‘é‡ ...")
    all_vecs = []
    with torch.no_grad():
        for i in range(0, len(texts), batch_size):
            batch = texts[i:i+batch_size]
            enc = tokenizer(batch, return_tensors="pt", padding=True, truncation=True, max_length=max_len)
            enc = {k: v.to(device) for k, v in enc.items()}
            out = model(**enc)

            # å…¼å®¹ text2vecï¼ˆsentence embeddingï¼‰å’Œæ™®é€š BERT
            if hasattr(out, "pooler_output") and out.pooler_output is not None:
                cls = out.pooler_output.detach().cpu().numpy()
            else:
                cls = out.last_hidden_state[:, 0, :].detach().cpu().numpy()

            all_vecs.append(cls)
    return np.vstack(all_vecs)

# =============== æ¨¡å‹è®­ç»ƒä¸è¯„ä¼° ===============
def pearson_corr(a, b):
    return np.corrcoef(a, b)[0, 1] if len(a) > 1 else np.nan

def train_and_evaluate(X, y, model_name):
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=SEED)

    model = LGBMRegressor(
        n_estimators=500,
        learning_rate=0.05,
        max_depth=-1,
        random_state=SEED,
        n_jobs=-1
    )
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    rmse = np.sqrt(mean_squared_error(y_test, y_pred))
    mae  = mean_absolute_error(y_test, y_pred)
    r2   = r2_score(y_test, y_pred)
    corr = pearson_corr(y_test, y_pred)

    return {
        "æ¨¡å‹": model_name,
        "RMSEâ†“": rmse,
        "MAEâ†“": mae,
        "R2â†‘": r2,
        "Pearsonâ†‘": corr
    }

# =============== ä¸»å…¥å£ ===============
def main():
    texts, y = load_data(DATA_PATH)
    all_results = []

    for model_name in MODEL_LIST:
        X = get_embeddings(texts, model_name)
        if X is None:  # è·³è¿‡å¤±è´¥çš„æ¨¡å‹
            continue
        try:
            results = train_and_evaluate(X, y, model_name)
            all_results.append(results)
            print(f"âœ… {model_name} è¯„ä¼°å®Œæˆ: R2={results['R2â†‘']:.4f}")
        except Exception as e:
            print(f"âŒ {model_name} è®­ç»ƒ/è¯„ä¼°å¤±è´¥: {e}")
            continue

    # ä¿å­˜æ‰€æœ‰ç»“æœ
    if all_results:
        df_results = pd.DataFrame(all_results)
        df_results.to_csv(SAVE_SUMMARY_PATH, index=False, encoding="utf-8-sig")
        print("\n=== æ‰€æœ‰æ¨¡å‹å¯¹æ¯”ç»“æœ ===")
        print(df_results.to_string(index=False))
        print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜è‡³: {os.path.abspath(SAVE_SUMMARY_PATH)}")
    else:
        print("âš ï¸ æ²¡æœ‰æˆåŠŸè¿è¡Œçš„æ¨¡å‹")

if __name__ == "__main__":
    main()
